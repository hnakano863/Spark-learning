#+title: Getting Started with Pyspark
#+property: header-args :eval no-export

* Quick Start
** Spark Shellへ入る
コマンドラインで ~pyspark~ と打てばシェルへ入れる

#+begin_src shell
$ pyspark
#+end_src

#+begin_example
Python 3.7.11 (default, Jun 28 2021, 17:43:59)
[GCC 10.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
21/09/21 11:38:30 WARN Utils: Your hostname, DESKTOP-ACDI00P resolves to a loopback address: 127.0.1.1; using 172.25.92.75 instead (on interface eth0)
21/09/21 11:38:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/09/21 11:38:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/09/21 11:38:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Python version 3.7.11 (default, Jun 28 2021 17:43:59)
SparkSession available as 'spark'.
>>>
#+end_example

*** Scalaの場合
~pyspark~ ではなく ~spark-shell~ というコマンドでscalaのシェルに入る。

** Spark Shell から抜ける
Spark Shell は普通のPython REPL と同じなので、 ~exit()~ で抜けられる。

#+begin_src shell
>>> exit()
#+end_src

*** Scalaの場合
scalaのシェルなので、 ~:quit~ で抜ける。

** Datasetオブジェクト
DatasetオブジェクトはSparkの分散データオブジェクトの一つ。

Scalaの場合はデータの型が定まる。
#+begin_src shell
scala> val textFile = spark.read.textFile("data/README.md")
textFile: org.apache.spark.sql.Dataset[String] = [value: string]
#+end_src

#+begin_src shell
scala> textFile.show(5)
#+end_src

#+begin_example
+--------------------+
|               value|
+--------------------+
|      # Apache Spark|
|                    |
|Spark is a unifie...|
|high-level APIs i...|
|supports general ...|
+--------------------+
only showing top 5 rows
#+end_example

しかし、Pythonの場合は型がないので、Scalaでいうところの ~Dataset[Row]~ という型を ~DataFrame~ 型として提供している。
これは、 ~Dataset~ の要素の型を何も決めていないものと考えればよい。

#+begin_src shell
>>> textFile = spark.read.text("data/README.md")
>>> type(textFile)
<class 'pyspark.sql.dataframe.DataFrame'>
#+end_src

#+begin_src shell
>>> from pyspark.sql.dataframe import DataFrame
>>> help(DataFrame)
#+end_src

#+begin_example
Help on class DataFrame in module pyspark.sql.dataframe:

class DataFrame(builtins.object)
 |  DataFrame(jdf, sql_ctx)
 |
 |  A distributed collection of data grouped into named columns.
 |
 |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,
 |  and can be created using various functions in :class:`SparkSession`::
 |
 |      people = spark.read.parquet("...")
 |
 |  Once created, it can be manipulated using the various domain-specific-language
 |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.
 |
 |  To select a column from the data frame, use the apply method::
 |
 |      ageCol = people.age
 |
 |  A more concrete example::
 |  ...
#+end_example
