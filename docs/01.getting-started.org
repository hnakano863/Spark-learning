#+title: Getting Started with Pyspark
#+property: header-args :eval no-export

* Quick Start
** Spark Shellへ入る
コマンドラインで ~pyspark~ と打てばシェルへ入れる

#+begin_src shell
$ pyspark
#+end_src

#+begin_example
Python 3.7.11 (default, Jun 28 2021, 17:43:59)
[GCC 10.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
21/09/21 11:38:30 WARN Utils: Your hostname, DESKTOP-ACDI00P resolves to a loopback address: 127.0.1.1; using 172.25.92.75 instead (on interface eth0)
21/09/21 11:38:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/09/21 11:38:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/09/21 11:38:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Python version 3.7.11 (default, Jun 28 2021 17:43:59)
SparkSession available as 'spark'.
>>>
#+end_example

*** Scalaの場合
~pyspark~ ではなく ~spark-shell~ というコマンドでscalaのシェルに入る。

** Spark Shell から抜ける
Spark Shell は普通のPython REPL と同じなので、 ~exit()~ で抜けられる。

#+begin_src shell
>>> exit()
#+end_src

*** Scalaの場合
scalaのシェルなので、 ~:quit~ で抜ける。

** Datasetオブジェクト
DatasetオブジェクトはSparkの分散データオブジェクトの一つ。

Scalaの場合はデータの型が定まる。
#+begin_src shell
scala> val textFile = spark.read.textFile("data/README.md")
textFile: org.apache.spark.sql.Dataset[String] = [value: string]
#+end_src

#+begin_src shell
scala> textFile.show(5)
#+end_src

#+begin_example
+--------------------+
|               value|
+--------------------+
|      # Apache Spark|
|                    |
|Spark is a unifie...|
|high-level APIs i...|
|supports general ...|
+--------------------+
only showing top 5 rows
#+end_example

しかし、Pythonの場合は型がないので、Scalaでいうところの ~Dataset[Row]~ という型を ~DataFrame~ 型として提供している。
これは、 ~Dataset~ の要素の型を何も決めていないものと考えればよい。

#+begin_src shell
>>> textFile = spark.read.text("data/README.md")
>>> type(textFile)
<class 'pyspark.sql.dataframe.DataFrame'>
#+end_src

#+begin_src shell
>>> from pyspark.sql.dataframe import DataFrame
>>> help(DataFrame)
#+end_src

#+begin_example
Help on class DataFrame in module pyspark.sql.dataframe:

class DataFrame(builtins.object)
 |  DataFrame(jdf, sql_ctx)
 |
 |  A distributed collection of data grouped into named columns.
 |
 |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,
 |  and can be created using various functions in :class:`SparkSession`::
 |
 |      people = spark.read.parquet("...")
 |
 |  Once created, it can be manipulated using the various domain-specific-language
 |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.
 |
 |  To select a column from the data frame, use the apply method::
 |
 |      ageCol = people.age
 |
 |  A more concrete example::
 |  ...
#+end_example

** 簡単なデータ操作とPandasの操作との対応
*** 行数のカウント
**** Spark
#+begin_src python
sparkDF.count()
#+end_src

**** Pandas
#+begin_src python
len(pandasDF)
#+end_src

*** 最初のレコードの取得
**** Spark
#+begin_src python
sparkDF.first()
#+end_src

**** Pandas
#+begin_src python
pandasDF.head(1)
#+end_src

*** 最初のnレコードの表示
**** Spark
#+begin_src python
sparkDF.show(n)
#+end_src

引数なしでも可能であり、その場合は ~n=20~ として実行される。

**** Pandas
#+begin_src python
pandasDF.head(n)
#+end_src

引数なしでも可能であり、その場合は ~n=5~ として実行される。

*** クエリ
列名 ~value~ の列に文字列データを格納したデータフレームが与えられているとする。
~value~ 列に文字列 "Spark" を含むレコードのみ抽出したいとする。

**** Spark
#+begin_src python
sparkDF.filter(sparkDF.value.contains("Spark"))
#+end_src

**** Pandas
#+begin_src python
pandasDF.loc[pandasDF.value.str.contains("Spark"), :]
#+end_src

**** 共通の考え方
Sparkにしろ、Pandasにしろ、まずクエリ条件の対象となる列を取り出して、それを ~bool~ 列に変換する。
そのあと、 ~bool~ 列の値が ~True~ となるような行だけを取得する。

ScalaやJuliaの場合は、このような考え方ではなく、要素の値に対してブールを返す関数を使ってクエリする。

例えばScalaのSparkで同じクエリを書く場合は、
#+begin_src shell
scala> textFile.filter(line => line.contains("Spark"))
res1: org.apache.spark.sql.Dataset[String] = [value: string]
#+end_src

Juliaの ~DataFrames.jl~ を使う場合は、
#+begin_src julia
filter(:value => (x -> occursin("Spark", x)), juliaDF)
#+end_src


* 参考
** http://spark.apache.org/docs/latest/quick-start.html
