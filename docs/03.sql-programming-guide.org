#+title: SQL Programming Guide
#+property: header-args :eval no-export

* Spark SQL の概要
Spark SQL は構造化データを処理するためのモジュール。

利用するAPIや言語に関わらず、データ処理には同じエンジンが使われる。
従って、必要に応じて最適なAPIに切り替えることが可能。

** DatasetとDataFrameについて
DatasetはSpark1.6以降に追加されたインターフェイス。
RDDの強い型付けと無名関数を使った柔軟な処理をSpark SQLの最適化エンジンと組合せて利用できるようにしたもの。
完全なDataset API はJavaとScalaからしか利用できないけど、PythonやRからでも、大抵の利点を享受できる。

DataFrameは名前付きの列で構成されたDatasetのこと。
R/PythonのDataFrameや関係データベースのテーブルと同じようなもの。
ScalaやJavaでは Dataset[Row]のエイリアスである。

DataFrame API はSpark, Java, Python, Rから利用できる。
このドキュメントでは主にPythonでDataFrame API を利用するのに必要な知識に焦点を絞っていこうと思う。

* SparkSession
Spark SQL の機能を利用するには、まず ~SparkSession~ を作成しないといけない。
インスタンス化には ~SparkSession.builder~ を使う。

** SparkSessionのドキュメンテーション
#+begin_src python
help(SparkSession)
#+end_src

#+begin_example
class SparkSession(builtins.object)
 |  SparkSession(sparkContext, jsparkSession=None)
 |
 |  The entry point to programming Spark with the Dataset and DataFrame API.
 |
 |  A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as |  tables, execute SQL over tables, cache tables, and read parquet files.
 |  To create a SparkSession, use the following builder pattern:
 |
 |  >>> spark = SparkSession.builder \
 |  ...     .master("local") \
 |  ...     .appName("Word Count") \
 |  ...     .config("spark.some.config.option", "some-value") \
 |  ...     .getOrCreate()
 |

#+end_example

#+begin_src python
help(SparkSession.builder)
#+end_src

#+begin_example
Help on Builder in module pyspark.sql.session object:

class Builder(builtins.object)
 |  Builder for :class:`SparkSession`.
 |
 |  Methods defined here:
 |
 |  appName(self, name)
 |      Sets a name for the application, which will be shown in the Spark web UI.
 |
 |      If no application name is set, a randomly generated name will be used.
 |
 |      :param name: an application name
 |
 |      .. versionadded:: 2.0
 |
 |  config(self, key=None, value=None, conf=None)
 |      Sets a config option. Options set using this method are automatically propagated to
 |      both :class:`SparkConf` and :class:`SparkSession`'s own configuration.
 |
 |      For an existing SparkConf, use `conf` parameter.
 |
 |      >>> from pyspark.conf import SparkConf
 |      >>> SparkSession.builder.config(conf=SparkConf())
 |      <pyspark.sql.session...
 |
 |      For a (key, value) pair, you can omit parameter names.
 |
 |      >>> SparkSession.builder.config("spark.some.config.option", "some-value")
 |      <pyspark.sql.session...
 |
 |      :param key: a key name string for configuration property
 |      :param value: a value for configuration property
 |      :param conf: an instance of :class:`SparkConf`
 |
 |      .. versionadded:: 2.0
 |
 |  enableHiveSupport(self)
 |      Enables Hive support, including connectivity to a persistent Hive metastore, support
 |      for Hive serdes, and Hive user-defined functions.
 |
 |      .. versionadded:: 2.0
 |
 |  getOrCreate(self)
 |      Gets an existing :class:`SparkSession` or, if there is no existing one, creates a
 |      new one based on the options set in this builder.
 |
 |      This method first checks whether there is a valid global default SparkSession, and if
 |      yes, return that one. If no valid global default SparkSession exists, the method
 |      creates a new SparkSession and assigns the newly created SparkSession as the global
 |      default.
 |
 |      >>> s1 = SparkSession.builder.config("k1", "v1").getOrCreate()
 |      >>> s1.conf.get("k1") == s1.sparkContext.getConf().get("k1") == "v1"
 |      True
 |
 |      In case an existing SparkSession is returned, the config options specified
 |      in this builder will be applied to the existing SparkSession.
 |
 |      >>> s2 = SparkSession.builder.config("k2", "v2").getOrCreate()
 |      >>> s1.conf.get("k1") == s2.conf.get("k1")
 |      True
 |
 |      .. versionadded:: 2.0
 |
 |  master(self, master)
 |      Sets the Spark master URL to connect to, such as "local" to run locally, "local[4]"
 |      to run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone
 |      cluster.
 |
 |      :param master: a url for spark master
 |
 |      .. versionadded:: 2.0
 |
#+end_example

** ドキュメンテーションの要約
いちばん簡単なインスタンス化は、 ~SparkSession.builder.getOrCreate()~ である。

app nameを設定して作成したければ、 ~SparkSession.builder.appName("appName").getOrCreate()~
なんらかのコンフィグをカスタムで設定したければ、

#+begin_src python
spark = SparkSession.builder\
    .appName("appName")\
    .config("some.spark.config.option", "your config value")\
    .getOrCreate()
#+end_src

さらに、既になんらかの設定が ~SparkConf~ インスタンスとして存在するならば、

#+begin_src python
# SparkConfインスタンスconfが存在するものとする。
spark = SparkSession.builder\
    .config(conf=conf)\
    .getOrCreate()
#+end_src

としてインスタンス化できる。

* DataFrameの作成方法
spark-shell や pyspark のshellに入ったときは、基本的に ~spark~ という名前の ~SparkSession~ インスタンスがすでに用意されている。
~SparkSession~ をエントリーポイントとして、RDD, Hiveテーブルやその他の spark data sourceからDataFrameを作成できる。
spark data sourcesの詳細については [[http://spark.apache.org/docs/latest/sql-data-sources.html][こちら]] を参照。

#+begin_src python
df = spark.read.json('result/share/people.json')
df.show()
#+end_src

#+begin_example
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+
#+end_example

* 基本的なオペレーションとpandasとの比較
** 列へのアクセス
~df.age~ または ~df['age']~
~df['age']~ の方が推奨らしい

pandasの場合も同じ記法で列アクセスできる。

*** 列の型
なお、SparkのDataFrameにおいて、列の型は ~Column~ 型になる。

#+begin_src python
type(df['age'])
#+end_src

#+begin_example
<class 'pyspark.sql.column.Column'>
#+end_example

#+begin_src python
help(df['age'])
#+end_src

#+begin_example
Help on Column in module pyspark.sql.column object:

class Column(builtins.object)
 |  Column(jc)
 |
 |  A column in a DataFrame.
 |
 |  :class:`Column` instances can be created by::
 |
 |      # 1. Select a column out of a DataFrame
 |
 |      df.colName
 |      df["colName"]
 |
 |      # 2. Create from an expression
 |      df.colName + 1
 |      1 / df.colName
 |
 |  .. versionadded:: 1.3
 |
#+end_example

** 列の選択
~DataFrame.select~ を使う。返り値は ~DataFrame~ になる。

#+begin_src python
x = df.select('name')
x.show()
#+end_src

#+begin_example
+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
+-------+
#+end_example

#+begin_src python
type(x)
#+end_src

#+begin_example
<class 'pyspark.sql.dataframe.DataFrame'>
#+end_example

この操作はpandasでは ~df[['name']]~ に当たる。

*** selectのtips

引数には複数の列名や ~*~ , ~Column~ 型の値やその混合も可能である。
#+begin_src python
df.select('name', 'age')
df.select('*')
df.select(df['name'])
df.select('name', df['age'], '*')
#+end_src

また、selectは列の変形を同時に行うことができる。

#+begin_src python
df.select('name', df['age'] + 1).show()
#+end_src

#+begin_example
+-------+---------+
|   name|(age + 1)|
+-------+---------+
|Michael|     null|
|   Andy|       31|
| Justin|       20|
+-------+---------+
#+end_example

*** selectのドキュメント
#+begin_src python
help(df.select)
#+end_src

#+begin_example
Help on method select in module pyspark.sql.dataframe:

select(*cols) method of pyspark.sql.dataframe.DataFrame instance
    Projects a set of expressions and returns a new :class:`DataFrame`.

    :param cols: list of column names (string) or expressions (:class:`Column`).
        If one of the column names is '*', that column is expanded to include all columns
        in the current DataFrame.

    >>> df.select('*').collect()
    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]
    >>> df.select('name', 'age').collect()
    [Row(name='Alice', age=2), Row(name='Bob', age=5)]
    >>> df.select(df.name, (df.age + 10).alias('age')).collect()
    [Row(name='Alice', age=12), Row(name='Bob', age=15)]

    .. versionadded:: 1.3
#+end_example

** フィルタ
~DataFrame.filter~ を使って行の値についてレコードをフィルタできる。

#+begin_src python
df.filter(df['age'] > 21).show()
#+end_src

#+begin_example
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+
#+end_example

pandasだと、以下の2つが相同

#+begin_src python
df.loc[df['age'] > 21, :]
df.query('age > 21')
#+end_src

~DataFrame.filter~ の引数はブール値の ~Column~ 型と決まっているので、
SQLの ~WHERE~ や pandas の ~query~ のつもりで使うよりも、 pandasの ~loc~ のつもりで使う方が良いかもしれない。

*** filterのドキュメント
#+begin_src python
help(df.filter)
#+end_src

#+begin_example
Help on method filter in module pyspark.sql.dataframe:

filter(condition) method of pyspark.sql.dataframe.DataFrame instance
    Filters rows using the given condition.

    :func:`where` is an alias for :func:`filter`.

    :param condition: a :class:`Column` of :class:`types.BooleanType`
        or a string of SQL expression.

    >>> df.filter(df.age > 3).collect()
    [Row(age=5, name='Bob')]
    >>> df.where(df.age == 2).collect()
    [Row(age=2, name='Alice')]

    >>> df.filter("age > 3").collect()
    [Row(age=5, name='Bob')]
    >>> df.where("age = 2").collect()
    [Row(age=2, name='Alice')]

    .. versionadded:: 1.3
#+end_example

** groupby
#+begin_src python
df.groupBy('age').count().show()
#+end_src

#+begin_example
+----+-----+
| age|count|
+----+-----+
|  19|    1|
|null|    1|
|  30|    1|
+----+-----+
#+end_example

pandasだと、少し長いが、以下のコードと相同になる。
インデックスができないことや列名が変わることに注意

#+begin_src python
df.groupby('age', as_index=False)\
    .name.count()\
    .rename(columns={'name': 'count'})
#+end_src

*** groupByの型
~DataFrame.groupBy~ の返り値は ~GroupedData~ クラスになる。

#+begin_src python
gdf = df.groupBy('age')
type(gdf)
#+end_src

#+begin_example
<class 'pyspark.sql.group.GroupedData'>
#+end_example

#+begin_src python
help(gdf)
#+end_src

#+begin_example
Help on GroupedData in module pyspark.sql.group object:

class GroupedData(builtins.object)
 |  GroupedData(jgd, df)
 |
 |  A set of methods for aggregations on a :class:`DataFrame`,
 |  created by :func:`DataFrame.groupBy`.
#+end_example

*** GroupedDataに対して渡せる集約関数
~GroupedData~ クラスのドキュメントの続きに書いてある。

#+begin_quote
1. built-in aggregation functions, such as `avg`, `max`, `min`, `sum`, `count`
2. group aggregate pandas UDFs, created with :func:`pyspark.sql.functions.pandas
#+end_quote

2については後述する。

** その他のDataFrame API
以下を参照
http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis

また、列に対するオペレーションについては、以下を参照
http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions

* 参考
** http://spark.apache.org/docs/latest/sql-getting-started.html
